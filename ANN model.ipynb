{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Kiran Shankar Bhat/Documents/DATA/data/BankNote_Authentication.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6860"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('class',axis=1)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "scaled_x = scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12180565,  1.14945512, -0.97597007,  0.35456135],\n",
       "       [ 1.44706568,  1.06445293, -0.89503626, -0.12876744],\n",
       "       [ 1.20780971, -0.77735215,  0.12221838,  0.61807317],\n",
       "       ...,\n",
       "       [-1.47235682, -2.62164576,  3.75901744, -0.75488418],\n",
       "       [-1.40669251, -1.75647104,  2.552043  , -0.04315848],\n",
       "       [-1.04712236, -0.43982168,  0.29861555,  1.1364645 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(scaled_x , columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.121806</td>\n",
       "      <td>1.149455</td>\n",
       "      <td>-0.975970</td>\n",
       "      <td>0.354561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.447066</td>\n",
       "      <td>1.064453</td>\n",
       "      <td>-0.895036</td>\n",
       "      <td>-0.128767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.207810</td>\n",
       "      <td>-0.777352</td>\n",
       "      <td>0.122218</td>\n",
       "      <td>0.618073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.063742</td>\n",
       "      <td>1.295478</td>\n",
       "      <td>-1.255397</td>\n",
       "      <td>-1.144029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036772</td>\n",
       "      <td>-1.087038</td>\n",
       "      <td>0.736730</td>\n",
       "      <td>0.096587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>-0.009711</td>\n",
       "      <td>-0.097693</td>\n",
       "      <td>-0.660962</td>\n",
       "      <td>0.300996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-0.641313</td>\n",
       "      <td>-1.158984</td>\n",
       "      <td>1.179023</td>\n",
       "      <td>0.730127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-1.472357</td>\n",
       "      <td>-2.621646</td>\n",
       "      <td>3.759017</td>\n",
       "      <td>-0.754884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-1.406693</td>\n",
       "      <td>-1.756471</td>\n",
       "      <td>2.552043</td>\n",
       "      <td>-0.043158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-1.047122</td>\n",
       "      <td>-0.439822</td>\n",
       "      <td>0.298616</td>\n",
       "      <td>1.136464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis   entropy\n",
       "0     1.121806  1.149455 -0.975970  0.354561\n",
       "1     1.447066  1.064453 -0.895036 -0.128767\n",
       "2     1.207810 -0.777352  0.122218  0.618073\n",
       "3     1.063742  1.295478 -1.255397 -1.144029\n",
       "4    -0.036772 -1.087038  0.736730  0.096587\n",
       "...        ...       ...       ...       ...\n",
       "1367 -0.009711 -0.097693 -0.660962  0.300996\n",
       "1368 -0.641313 -1.158984  1.179023  0.730127\n",
       "1369 -1.472357 -2.621646  3.759017 -0.754884\n",
       "1370 -1.406693 -1.756471  2.552043 -0.043158\n",
       "1371 -1.047122 -0.439822  0.298616  1.136464\n",
       "\n",
       "[1372 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1097, 4), (1097,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((275, 4), (275,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model.add(Dense(8, activation = 'relu', input_shape=(x_train.shape[1],)))\n",
    "relu_model.add(Dense(10,activation= 'relu'))\n",
    "relu_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam , SGD , RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 0.5862 - accuracy: 0.6716 - val_loss: 0.4552 - val_accuracy: 0.8364\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.9065 - val_loss: 0.2794 - val_accuracy: 0.9591\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9532 - val_loss: 0.1578 - val_accuracy: 0.9773\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9772 - val_loss: 0.0993 - val_accuracy: 0.9864\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9875 - val_loss: 0.0673 - val_accuracy: 0.9864\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9909 - val_loss: 0.0454 - val_accuracy: 0.9909\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9989 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.1889e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.7050e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 7.6700e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 6.9709e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 9.3980e-04 - accuracy: 1.0000 - val_loss: 6.4038e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 8.9367e-04 - accuracy: 1.0000 - val_loss: 6.1601e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 8.1718e-04 - accuracy: 1.0000 - val_loss: 5.8732e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 7.6712e-04 - accuracy: 1.0000 - val_loss: 5.3578e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 7.0444e-04 - accuracy: 1.0000 - val_loss: 4.7229e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 6.7092e-04 - accuracy: 1.0000 - val_loss: 4.4007e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 6.2058e-04 - accuracy: 1.0000 - val_loss: 4.3403e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.8624e-04 - accuracy: 1.0000 - val_loss: 3.7903e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.6257e-04 - accuracy: 1.0000 - val_loss: 3.5531e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.1834e-04 - accuracy: 1.0000 - val_loss: 3.2561e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4.8474e-04 - accuracy: 1.0000 - val_loss: 3.1552e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4.5524e-04 - accuracy: 1.0000 - val_loss: 2.9033e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "predict = relu_model.fit(x_train, y_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=10,\n",
    "               epochs=50,\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here for the \"relu\" Activation function we got\n",
    "+ Trining loss as: 0.00045\n",
    "+ Testing  loss as: 0.00029\n",
    "+ We can say that the model is quite underfiting.\n",
    "+ Which are not too close and this model may not work well for noisy and large data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.27452937,  0.15015186,  0.42846608, -0.25975686,  0.5370475 ,\n",
       "          0.19427927,  0.9403409 ,  0.20973358,  0.06888471, -0.26273268],\n",
       "        [ 0.47251624, -0.34081507, -0.0793559 ,  0.29571098,  0.09074057,\n",
       "         -0.09786923, -0.06938271,  0.6690126 , -0.0823969 ,  0.4239281 ],\n",
       "        [ 0.89389807,  0.91833514, -0.1548142 ,  1.0502833 , -0.4202236 ,\n",
       "         -0.10043642, -0.12883961, -0.60643965, -0.4511721 ,  0.5584125 ],\n",
       "        [-0.22383703,  0.27987286,  0.00377816,  0.22476538,  0.00662109,\n",
       "          0.25786328, -0.01836834, -0.13360068, -0.4181032 ,  0.62398577],\n",
       "        [ 0.34900352,  0.3805087 , -1.074349  ,  0.7879395 , -0.97438276,\n",
       "          0.8050783 , -0.49340945, -0.73291445,  0.35850286,  0.42289194],\n",
       "        [-0.022984  , -0.11002689,  1.0033605 , -0.31985977,  1.1266446 ,\n",
       "         -0.14402089,  0.68403935,  0.82269895,  0.32750544, -0.36830932],\n",
       "        [-0.28810918,  0.02331265,  0.5923757 , -0.23776795,  0.44631302,\n",
       "         -0.17779426,  0.65541667,  0.28145048, -0.18904302, -0.04962824],\n",
       "        [ 0.71343523,  0.23611134,  0.05206382,  0.48810643,  0.04447332,\n",
       "          0.39162773, -0.25006273,  0.0634912 , -0.11331281, -0.04743973]],\n",
       "       dtype=float32),\n",
       " array([-0.21891248, -0.15642701,  0.68400824,  0.00198422,  0.7913218 ,\n",
       "        -0.16015267,  0.5194267 ,  0.84107965, -0.19026709, -0.0889419 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "relu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = relu_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6316321e-10],\n",
       "       [2.4487972e-03],\n",
       "       [1.8370854e-05],\n",
       "       [3.0102727e-17],\n",
       "       [1.5752456e-10],\n",
       "       [4.1763670e-07],\n",
       "       [3.6945409e-11],\n",
       "       [3.3077092e-09],\n",
       "       [1.2496058e-07],\n",
       "       [2.2356568e-11],\n",
       "       [9.9987173e-01],\n",
       "       [9.9998885e-01],\n",
       "       [1.1213268e-09],\n",
       "       [9.9991453e-01],\n",
       "       [3.0053183e-08],\n",
       "       [9.9998045e-01],\n",
       "       [9.9994230e-01],\n",
       "       [9.9995369e-01],\n",
       "       [9.9999976e-01],\n",
       "       [9.9961448e-01],\n",
       "       [3.4343306e-11],\n",
       "       [7.4024475e-10],\n",
       "       [9.9994504e-01],\n",
       "       [1.7877085e-11],\n",
       "       [1.0000000e+00],\n",
       "       [5.8617633e-10],\n",
       "       [7.0033280e-13],\n",
       "       [9.9998546e-01],\n",
       "       [4.3431724e-15],\n",
       "       [2.8656260e-13],\n",
       "       [9.9999958e-01],\n",
       "       [1.9435453e-09],\n",
       "       [1.2280287e-07],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [4.8334440e-12],\n",
       "       [9.9977487e-01],\n",
       "       [9.9730808e-01],\n",
       "       [9.9997896e-01],\n",
       "       [3.5963427e-10],\n",
       "       [4.4360036e-11],\n",
       "       [9.9999964e-01],\n",
       "       [9.9993098e-01],\n",
       "       [5.8015186e-11],\n",
       "       [9.9999774e-01],\n",
       "       [9.9999559e-01],\n",
       "       [9.9993646e-01],\n",
       "       [1.3841687e-06],\n",
       "       [2.3811010e-08],\n",
       "       [9.9999452e-01],\n",
       "       [5.5431892e-06],\n",
       "       [1.1961326e-09],\n",
       "       [4.8750804e-12],\n",
       "       [4.2538507e-11],\n",
       "       [1.3040334e-09],\n",
       "       [9.9997520e-01],\n",
       "       [2.4981460e-14],\n",
       "       [3.4499873e-12],\n",
       "       [1.5297538e-06],\n",
       "       [2.1284491e-12],\n",
       "       [9.9999845e-01],\n",
       "       [1.2855413e-09],\n",
       "       [9.9999088e-01],\n",
       "       [2.2335436e-14],\n",
       "       [5.4249723e-16],\n",
       "       [1.3751799e-11],\n",
       "       [5.5737270e-10],\n",
       "       [1.3532563e-10],\n",
       "       [3.1752270e-06],\n",
       "       [9.9999285e-01],\n",
       "       [9.9998105e-01],\n",
       "       [1.4440303e-10],\n",
       "       [9.9981689e-01],\n",
       "       [2.7703574e-11],\n",
       "       [9.9288142e-01],\n",
       "       [9.8608136e-03],\n",
       "       [2.6047537e-06],\n",
       "       [9.9999166e-01],\n",
       "       [9.9997950e-01],\n",
       "       [9.9949646e-01],\n",
       "       [9.9958926e-01],\n",
       "       [4.2915344e-04],\n",
       "       [9.9999875e-01],\n",
       "       [1.6731143e-11],\n",
       "       [6.9077087e-06],\n",
       "       [2.0530540e-14],\n",
       "       [4.2885306e-11],\n",
       "       [9.9999857e-01],\n",
       "       [9.9979651e-01],\n",
       "       [7.1442290e-11],\n",
       "       [3.4512657e-11],\n",
       "       [6.0497157e-10],\n",
       "       [9.9417329e-01],\n",
       "       [9.9999028e-01],\n",
       "       [3.3635778e-11],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [1.3326384e-06],\n",
       "       [5.8278590e-11],\n",
       "       [3.8648573e-06],\n",
       "       [1.0000000e+00],\n",
       "       [2.5412294e-07],\n",
       "       [1.7606147e-10],\n",
       "       [2.1326605e-07],\n",
       "       [1.0000000e+00],\n",
       "       [1.8766910e-09],\n",
       "       [7.8755747e-08],\n",
       "       [9.9999470e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999511e-01],\n",
       "       [9.9999332e-01],\n",
       "       [9.9996543e-01],\n",
       "       [3.3465313e-14],\n",
       "       [9.9999881e-01],\n",
       "       [9.9955845e-01],\n",
       "       [9.9999982e-01],\n",
       "       [7.6411598e-13],\n",
       "       [9.9999976e-01],\n",
       "       [9.9999964e-01],\n",
       "       [2.9243052e-08],\n",
       "       [9.9999058e-01],\n",
       "       [9.0956238e-09],\n",
       "       [9.9937785e-01],\n",
       "       [5.2467313e-13],\n",
       "       [9.9971187e-01],\n",
       "       [2.6067077e-11],\n",
       "       [1.0000000e+00],\n",
       "       [9.9958253e-01],\n",
       "       [1.1308977e-09],\n",
       "       [9.9999785e-01],\n",
       "       [9.9999934e-01],\n",
       "       [3.6140312e-16],\n",
       "       [7.3043889e-12],\n",
       "       [6.8821919e-07],\n",
       "       [7.6426756e-13],\n",
       "       [1.3978549e-06],\n",
       "       [9.9999738e-01],\n",
       "       [2.9487951e-14],\n",
       "       [2.6894550e-10],\n",
       "       [1.0708630e-02],\n",
       "       [3.8913855e-10],\n",
       "       [3.9399611e-06],\n",
       "       [9.9997914e-01],\n",
       "       [3.8381132e-10],\n",
       "       [9.9999797e-01],\n",
       "       [9.9842948e-01],\n",
       "       [9.9994701e-01],\n",
       "       [9.9999785e-01],\n",
       "       [9.9999964e-01],\n",
       "       [3.9591982e-11],\n",
       "       [9.9999285e-01],\n",
       "       [9.9932325e-01],\n",
       "       [9.9999881e-01],\n",
       "       [2.9239449e-14],\n",
       "       [9.9971616e-01],\n",
       "       [3.3767311e-10],\n",
       "       [9.9998146e-01],\n",
       "       [3.7919906e-12],\n",
       "       [1.9582012e-10],\n",
       "       [2.5607476e-09],\n",
       "       [9.9999923e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999017e-01],\n",
       "       [9.9993181e-01],\n",
       "       [9.9931180e-01],\n",
       "       [2.3033356e-11],\n",
       "       [1.0000000e+00],\n",
       "       [2.7803343e-12],\n",
       "       [3.4169096e-12],\n",
       "       [8.9231406e-08],\n",
       "       [6.5777766e-07],\n",
       "       [1.4636355e-09],\n",
       "       [1.2803493e-08],\n",
       "       [9.9997211e-01],\n",
       "       [4.1224724e-10],\n",
       "       [1.2756345e-09],\n",
       "       [9.9998200e-01],\n",
       "       [9.9999964e-01],\n",
       "       [7.4424555e-10],\n",
       "       [1.0532615e-11],\n",
       "       [6.6465669e-05],\n",
       "       [4.1512598e-16],\n",
       "       [9.9999970e-01],\n",
       "       [8.7394415e-12],\n",
       "       [9.9999774e-01],\n",
       "       [2.2772784e-11],\n",
       "       [9.9985564e-01],\n",
       "       [9.9998248e-01],\n",
       "       [2.0698600e-13],\n",
       "       [8.5021899e-11],\n",
       "       [9.9990565e-01],\n",
       "       [5.1342632e-12],\n",
       "       [5.4835375e-10],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999917e-01],\n",
       "       [9.9796033e-01],\n",
       "       [9.9978924e-01],\n",
       "       [2.9898113e-09],\n",
       "       [7.1621947e-08],\n",
       "       [9.9999779e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999440e-01],\n",
       "       [5.4762188e-07],\n",
       "       [1.5783956e-16],\n",
       "       [9.9992609e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999863e-01],\n",
       "       [9.9999839e-01],\n",
       "       [2.1126993e-08],\n",
       "       [1.4216204e-07],\n",
       "       [4.2481422e-03],\n",
       "       [3.2790068e-14],\n",
       "       [9.8008346e-10],\n",
       "       [1.8112269e-06],\n",
       "       [5.8280446e-12],\n",
       "       [4.9983585e-12],\n",
       "       [9.8608136e-03],\n",
       "       [5.9215486e-09],\n",
       "       [9.9999952e-01],\n",
       "       [9.9999976e-01],\n",
       "       [9.9999523e-01],\n",
       "       [9.9999952e-01],\n",
       "       [9.9950159e-01],\n",
       "       [6.0767022e-08],\n",
       "       [9.9912667e-01],\n",
       "       [3.0762763e-08],\n",
       "       [1.2927682e-16],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999982e-01],\n",
       "       [9.9999565e-01],\n",
       "       [9.9995303e-01],\n",
       "       [1.1088925e-10],\n",
       "       [9.8136801e-01],\n",
       "       [1.0794611e-11],\n",
       "       [9.9999082e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9848962e-01],\n",
       "       [9.9999398e-01],\n",
       "       [4.3916981e-10],\n",
       "       [4.4169445e-07],\n",
       "       [6.0629141e-08],\n",
       "       [9.9985123e-01],\n",
       "       [9.6416807e-07],\n",
       "       [9.9999535e-01],\n",
       "       [9.9973476e-01],\n",
       "       [1.0000000e+00],\n",
       "       [3.6541630e-13],\n",
       "       [5.1478960e-11],\n",
       "       [1.0841201e-11],\n",
       "       [3.5563126e-08],\n",
       "       [2.4615442e-12],\n",
       "       [1.6261625e-08],\n",
       "       [9.9999249e-01],\n",
       "       [1.3700226e-05],\n",
       "       [9.9337733e-01],\n",
       "       [8.6281455e-07],\n",
       "       [1.1869827e-09],\n",
       "       [2.0116633e-10],\n",
       "       [9.9998379e-01],\n",
       "       [9.9999678e-01],\n",
       "       [1.1111627e-09],\n",
       "       [3.5597873e-06],\n",
       "       [2.9104164e-08],\n",
       "       [9.9993396e-01],\n",
       "       [9.9781287e-01],\n",
       "       [1.0559700e-09],\n",
       "       [9.9987334e-01],\n",
       "       [5.4125302e-08],\n",
       "       [1.0000000e+00],\n",
       "       [9.9995875e-01],\n",
       "       [9.9998915e-01],\n",
       "       [9.9885786e-01],\n",
       "       [9.4395309e-06],\n",
       "       [2.4485290e-03],\n",
       "       [2.4881944e-13]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = []\n",
    "for i in y_predict:\n",
    "    if i <0.5:\n",
    "        y_pre.append(0)\n",
    "    else:\n",
    "        y_pre.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148,   0],\n",
       "       [  0, 127]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       148\n",
      "           1       1.00      1.00      1.00       127\n",
      "\n",
      "    accuracy                           1.00       275\n",
      "   macro avg       1.00      1.00      1.00       275\n",
      "weighted avg       1.00      1.00      1.00       275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predict.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predict.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'],loc='upper left')\n",
    "plt.savefig('loss vs epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_model.add(Dense(8, activation=tf.keras.layers.LeakyReLU(), input_shape=(x_train.shape[1],)))\n",
    "leaky_model.add(Dense(10,activation= tf.keras.layers.LeakyReLU()))\n",
    "leaky_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.6418 - accuracy: 0.6568 - val_loss: 0.4386 - val_accuracy: 0.8591\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.9430 - val_loss: 0.2052 - val_accuracy: 0.9864\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9909 - val_loss: 0.1164 - val_accuracy: 0.9818\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0986 - accuracy: 0.9909 - val_loss: 0.0775 - val_accuracy: 0.9864\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9909 - val_loss: 0.0560 - val_accuracy: 0.9864\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9909 - val_loss: 0.0429 - val_accuracy: 0.9864\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9909 - val_loss: 0.0343 - val_accuracy: 0.9864\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9966 - val_loss: 0.0285 - val_accuracy: 0.9864\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9966 - val_loss: 0.0250 - val_accuracy: 0.9955\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9977 - val_loss: 0.0210 - val_accuracy: 0.9955\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9989 - val_loss: 0.0186 - val_accuracy: 0.9955\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9989 - val_loss: 0.0172 - val_accuracy: 0.9955\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9989 - val_loss: 0.0164 - val_accuracy: 0.9955\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9989 - val_loss: 0.0139 - val_accuracy: 0.9955\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9989 - val_loss: 0.0124 - val_accuracy: 0.9955\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9989 - val_loss: 0.0123 - val_accuracy: 0.9955\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9989 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9989 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 0.9955\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9989 - val_loss: 0.0095 - val_accuracy: 0.9955\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9989 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "predict = leaky_model.fit(x_train, y_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=10,\n",
    "               epochs=50,\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here for the lealyRelu activation function we got\n",
    "+ training loss as: 0.0016  \n",
    "+ testing loss as:0.0014, \n",
    "+ which is quite higher than the Relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model.add(Dense(8, activation= 'softmax', input_shape=(x_train.shape[1],)))\n",
    "softmax_model.add(Dense(10,activation= 'softmax'))\n",
    "softmax_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5656 - val_loss: 0.6827 - val_accuracy: 0.5364\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.6725 - accuracy: 0.5656 - val_loss: 0.6698 - val_accuracy: 0.5364\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.5656 - val_loss: 0.6484 - val_accuracy: 0.5364\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.5964 - val_loss: 0.6096 - val_accuracy: 0.7409\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.8848 - val_loss: 0.5606 - val_accuracy: 0.9091\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.9487 - val_loss: 0.5020 - val_accuracy: 0.9591\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.9669 - val_loss: 0.4418 - val_accuracy: 0.9636\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.9795 - val_loss: 0.3869 - val_accuracy: 0.9818\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.9863 - val_loss: 0.3394 - val_accuracy: 0.9955\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.9943 - val_loss: 0.2985 - val_accuracy: 0.9955\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9954 - val_loss: 0.2636 - val_accuracy: 0.9955\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9977 - val_loss: 0.2334 - val_accuracy: 0.9955\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9977 - val_loss: 0.2079 - val_accuracy: 0.9955\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 1.0000 - val_loss: 0.1670 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1590 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1034 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.0858 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0612 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "predict = softmax_model.fit(x_train, y_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=10,\n",
    "               epochs=50,\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here for the \"softmax\" activation function we got the \n",
    "+ traing loss as: 0.00033\n",
    "+ testing loss as: 0.00032, \n",
    "+ which are quite close and the quite less than that of both Relu and LeakyRelu activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_model.add(Dense(8, activation='tanh', input_shape=(x_train.shape[1],)))\n",
    "tan_model.add(Dense(10,activation= 'tanh'))\n",
    "tan_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.8016 - val_loss: 0.3481 - val_accuracy: 0.9091\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9373 - val_loss: 0.1850 - val_accuracy: 0.9818\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9829 - val_loss: 0.1055 - val_accuracy: 0.9864\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.9897 - val_loss: 0.0669 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9943 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9977 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9977 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9989 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9989 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 9.3862e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.8254e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.3255e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 7.7803e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 9.3249e-04 - accuracy: 1.0000 - val_loss: 7.3043e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 8.6398e-04 - accuracy: 1.0000 - val_loss: 6.6829e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 8.1062e-04 - accuracy: 1.0000 - val_loss: 6.3544e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 7.5888e-04 - accuracy: 1.0000 - val_loss: 5.9111e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 7.0883e-04 - accuracy: 1.0000 - val_loss: 5.5304e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 6.7336e-04 - accuracy: 1.0000 - val_loss: 5.1524e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 6.2172e-04 - accuracy: 1.0000 - val_loss: 4.8987e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.8718e-04 - accuracy: 1.0000 - val_loss: 4.5716e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 5.3443e-04 - accuracy: 1.00 - 0s 2ms/step - loss: 5.5076e-04 - accuracy: 1.0000 - val_loss: 4.3070e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 5.1835e-04 - accuracy: 1.0000 - val_loss: 4.0416e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4.8962e-04 - accuracy: 1.0000 - val_loss: 3.8035e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "predict = tan_model.fit(x_train, y_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=10,\n",
    "               epochs=50,\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here for the \"Tanh\" AF we got the \n",
    "+ training loss as:0.00035  \n",
    "+ testig loss as:0.00035, \n",
    "+ Which are quite close and quite less than Relu and LeakyRelu but quite higher than Softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
